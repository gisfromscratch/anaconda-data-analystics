{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.builtins import next\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Logging\n",
    "logging.getLogger().setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    # If data is missing, indicate that by setting the value to `None`\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "    \n",
    "    try : # python 2/3 string differences\n",
    "        column = column.decode('utf8')\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    # If data is missing, indicate that by setting the value to `None`\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "def readData(filename, encoding, delimiter, header, keyfield):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records, \n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename, encoding=encoding) as f:\n",
    "        reader = csv.DictReader(f, fieldnames=header, delimiter=delimiter, quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row[keyfield])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import training data\n",
    "filename = r'D:/Geonames/geonames_modifications.tsv'\n",
    "header = ['id', 'geonameid','name','asciiname','alternatenames','latitude','longitude','feature class','feature code','country code','cc2','admin1 code','admin2 code','admin3 code','admin4 code','population','elevation','dem','timezone','modification date']\n",
    "keyfield = 'id'\n",
    "trainingData = readData(filename, 'utf-8', '\\t', header, keyfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training\n",
    "fields = [\n",
    "    { 'field':'name', 'type':'String' },\n",
    "    { 'field':'asciiname', 'type':'String' },\n",
    "    { 'field':'latitude', 'type':'String' },\n",
    "    { 'field':'longitude', 'type':'String' },\n",
    "    { 'field':'country code', 'type':'Exact', 'has missing':True }\n",
    "]\n",
    "commonField = 'geonameid'\n",
    "\n",
    "# Create labeled data\n",
    "labeledData = dedupe.trainingDataDedupe(trainingData, commonField)\n",
    "\n",
    "# Create the matcher\n",
    "matcher = dedupe.Dedupe(fields)\n",
    "matcher.sample(trainingData)\n",
    "matcher.markPairs(labeledData)\n",
    "matcher.train()\n",
    "\n",
    "# When finished, save our training to disk\n",
    "trainingFile = r'D:/Geonames/geonames_modifications_training.json'\n",
    "with open(trainingFile, 'w') as tf:\n",
    "    matcher.writeTraining(tf)\n",
    "    \n",
    "# Save our weights and predicates to disk. If the settings file\n",
    "# exists, we will skip all the training and learning next time we run\n",
    "# this file.\n",
    "settingsFile = r'D:/Geonames/geonames_modifications.settings'\n",
    "with open(settingsFile, 'wb') as sf:\n",
    "    matcher.writeSettings(sf)\n",
    "    \n",
    "matcher.cleanupTraining()\n",
    "del matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 duplicates found.\n"
     ]
    }
   ],
   "source": [
    "# ## Import real data\n",
    "filename = r'D:/Geonames/cities1000.txt'\n",
    "header = ['geonameid','name','asciiname','alternatenames','latitude','longitude','feature class','feature code','country code','cc2','admin1 code','admin2 code','admin3 code','admin4 code','population','elevation','dem','timezone','modification date']\n",
    "keyfield = 'geonameid'\n",
    "geonames = readData(filename, 'utf-8', '\\t', header, keyfield)\n",
    "\n",
    "# Create the matcher from the settings file\n",
    "with open(settingsFile, 'rb') as f:\n",
    "    matcher = dedupe.StaticDedupe(f)\n",
    "    threshold = matcher.threshold(geonames)\n",
    "    matches = matcher.match(geonames, threshold)\n",
    "    print('%s duplicates found.' % len(matches))\n",
    "\n",
    "    del matcher\n",
    "\n",
    "def printMatches(matches):\n",
    "    for (clusterId, cluster) in enumerate(matches):\n",
    "        ids, scored = cluster\n",
    "        print(clusterId)\n",
    "        for id in ids:\n",
    "            print (geonames[id]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
